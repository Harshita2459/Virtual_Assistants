# -*- coding: utf-8 -*-
"""ML_MINI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16DmXRgutfYNuTPmazRRD8Gs2uJPeIwoH
"""

import pandas as pd

# File paths
file1_path = "/content/drive/MyDrive/crows_pairs_anonymized.csv"
file2_path = "/content/drive/MyDrive/prompts.csv"

# Load datasets
df_crows = pd.read_csv(file1_path)
df_prompts = pd.read_csv(file2_path)

# Display basic info about both datasets
df_crows.info()

df_prompts.info()

df_crows[0:10]

df_prompts[0:10]

# Drop unnecessary 'Unnamed: 0' columns
df_crows.drop(columns=['Unnamed: 0', 'annotations', 'anon_writer', 'anon_annotators'], inplace=True)
df_prompts.drop(columns=['Unnamed: 0'], inplace=True)

# Check for missing values
missing_crows = df_crows.isnull().sum()
missing_prompts = df_prompts.isnull().sum()

missing_crows, missing_prompts

"""#**Data Visualization**

"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")

# Countplot for bias type distribution
plt.figure(figsize=(10, 5))
sns.countplot(y=df_crows['bias_type'], order=df_crows['bias_type'].value_counts().index, palette="coolwarm")
plt.xlabel("Count")
plt.ylabel("Bias Type")
plt.title("Distribution of Bias Types in Crows-Pairs Dataset")
plt.show()

# Pie chart for stereo vs. antistereo labels
plt.figure(figsize=(6, 6))
df_crows['stereo_antistereo'].value_counts().plot.pie(autopct='%1.1f%%', colors=["#ff9999", "#66b3ff"], startangle=140)
plt.ylabel("")
plt.title("Stereotypical vs. Anti-Stereotypical Sentences")
plt.show()

!pip install aif360
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing

# Load dataset
df_crows = pd.read_csv("/content/drive/MyDrive/crows_pairs_anonymized.csv")  # Update path if needed

# Drop unnecessary text columns
df_crows = df_crows[['bias_type', 'stereo_antistereo']].copy()

# Encode 'stereo_antistereo' column (1 = stereotypical, 0 = anti-stereotypical)
df_crows['stereo_label'] = df_crows['stereo_antistereo'].apply(lambda x: 1 if x == 'stereo' else 0)

# Encode 'bias_type' column into numerical format
label_encoder = LabelEncoder()
df_crows['bias_type_encoded'] = label_encoder.fit_transform(df_crows['bias_type'])

# Keep only numerical columns for AIF360
df_numeric = df_crows[['stereo_label', 'bias_type_encoded']].copy()

# Convert to BinaryLabelDataset format
bias_dataset = BinaryLabelDataset(df=df_numeric,
                                  label_names=['stereo_label'],
                                  protected_attribute_names=['bias_type_encoded'])

# Compute Disparate Impact (measure of bias)
bias_metric = BinaryLabelDatasetMetric(bias_dataset,
                                       privileged_groups=[{'bias_type_encoded': 0}],  # Modify based on encoding
                                       unprivileged_groups=[{'bias_type_encoded': 1}])
disparate_impact_before = bias_metric.disparate_impact()

# Apply Reweighing (Bias Mitigation)
reweighing = Reweighing(unprivileged_groups=[{'bias_type_encoded': 1}],
                        privileged_groups=[{'bias_type_encoded': 0}])
transformed_dataset = reweighing.fit_transform(bias_dataset)

# Compute Disparate Impact after mitigation
bias_metric_transformed = BinaryLabelDatasetMetric(transformed_dataset,
                                                   privileged_groups=[{'bias_type_encoded': 0}],
                                                   unprivileged_groups=[{'bias_type_encoded': 1}])
disparate_impact_after = bias_metric_transformed.disparate_impact()

# Print results
print(f"Disparate Impact Before Mitigation: {disparate_impact_before}")
print(f"Disparate Impact After Mitigation: {disparate_impact_after}")

"""#**Fix dataset Imbalance**"""

# !pip install imbalanced-learn

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Load dataset
df_crows = pd.read_csv("/content/drive/MyDrive/crows_pairs_anonymized.csv")

# Encode categorical variables
df_crows['stereo_label'] = df_crows['stereo_antistereo'].apply(lambda x: 1 if x == 'stereo' else 0)
df_crows['bias_type_encoded'] = df_crows['bias_type'].astype('category').cat.codes  # Convert to numbers

# Select features and labels
X = df_crows[['bias_type_encoded']]
y = df_crows['stereo_label']

# Apply SMOTE for oversampling
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Print dataset shape before & after SMOTE
print("Original dataset shape:", X.shape)
print("Resampled dataset shape:", X_resampled.shape)

"""#**Train ML Model with Bias Mitigation**"""

# !pip install aif360 scikit-learn

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from aif360.metrics import ClassificationMetric
from aif360.datasets import BinaryLabelDataset

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Train model
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Convert test dataset to AIF360 format
test_dataset = BinaryLabelDataset(df=pd.DataFrame({'bias_type_encoded': X_test['bias_type_encoded'],
                                                   'stereo_label': y_test}),
                                  label_names=['stereo_label'],
                                  protected_attribute_names=['bias_type_encoded'])

# Convert predictions into BinaryLabelDataset format
pred_dataset = test_dataset.copy()
pred_dataset.labels = y_pred.reshape(-1, 1)

# Compute Fairness Metrics
fairness_metric = ClassificationMetric(test_dataset, pred_dataset,
                                       privileged_groups=[{'bias_type_encoded': 0}],
                                       unprivileged_groups=[{'bias_type_encoded': 1}])

print("Equalized Odds Difference:", fairness_metric.equal_opportunity_difference())
print("Demographic Parity Difference:", fairness_metric.disparate_impact())
print("Model Accuracy:", accuracy_score(y_test, y_pred))

"""#**Real-time Bias Detection Filter**"""

# !pip install transformers

from transformers import pipeline

# Load a pre-trained text classification pipeline
bias_detector = pipeline("text-classification", model="unitary/unbiased-toxic-roberta")

# Example inputs (Virtual Assistant responses)
responses = [
    "Women are not good at math.",
    "Black kids don’t have access to good education.",
    "Everyone deserves equal opportunities.",
    "White people have better IQ."
]

# Detect Bias
for text in responses:
    result = bias_detector(text)
    print(f"Response: {text} -> Bias Score: {result[0]['score']:.4f}")

"""#**Deploying Bias Detection in a Chatbot**"""

# !pip install transformers

from transformers import pipeline

# Load Bias Detection Model
bias_detector = pipeline("text-classification", model="unitary/unbiased-toxic-roberta")

def chatbot_response(user_input):
    # Detect Bias
    result = bias_detector(user_input)
    bias_score = result[0]['score']

    if bias_score > 0.5:  # Threshold for bias detection
        return "⚠️ This response might be biased. Please rephrase."
    else:
        return f"🤖 AI: {user_input}"  # Return unbiased response

# Test the chatbot
while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break
    print(chatbot_response(user_input))

# Check available columns
print("Columns in dataset:", df_crows.columns)

# If the column name is different, update it below
correct_column_name = 'stereo_antistereo'  # Change this if needed

# ✅ Convert categorical labels to numerical values
if correct_column_name in df_crows.columns:
    df_crows['stereo_label'] = df_crows[correct_column_name].apply(lambda x: 1 if x == 'stereo' else 0)
else:
    print(f"❌ Column '{correct_column_name}' not found in dataset!")

"""#**Improve Fairness with Adversarial Debiasing**"""

# ✅ Install dependencies
# !pip install tensorflow==2.15.0 aif360

# ✅ Suppress warnings
import warnings
warnings.filterwarnings("ignore")

# ✅ Import libraries
import pandas as pd
import numpy as np
import tensorflow as tf
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.inprocessing import AdversarialDebiasing

# ✅ Load dataset
df = pd.read_csv("/content/drive/MyDrive/crows_pairs_anonymized.csv")  # Modify filename if needed

# ✅ Print dataset columns
print("✔ Columns in dataset:", df.columns)

# ✅ Encode 'bias_type' as a numerical column (Protected Attribute)
df['bias_type_encoded'] = df['bias_type'].astype('category').cat.codes

# ✅ Convert 'stereo_antistereo' to binary labels (Target Attribute)
df['stereo_label'] = df['stereo_antistereo'].apply(lambda x: 1 if x == 'stereo' else 0)

# ✅ Ensure dataset has required columns
if 'bias_type_encoded' not in df.columns or 'stereo_label' not in df.columns:
    raise ValueError("❌ Dataset must have 'bias_type_encoded' and 'stereo_label' columns!")

# ✅ Split dataset into training and testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df[['bias_type_encoded']], df['stereo_label'], test_size=0.2, random_state=42)

# ✅ Construct train DataFrame
train_df = pd.DataFrame({
    'bias_type_encoded': X_train['bias_type_encoded'].astype(int),
    'stereo_label': y_train.astype(int)
})

# ✅ Create BinaryLabelDataset
train_dataset = BinaryLabelDataset(df=train_df,
                                   label_names=['stereo_label'],
                                   protected_attribute_names=['bias_type_encoded'])

# ✅ Fix TensorFlow 2.x Session issue
tf.compat.v1.reset_default_graph()  # ✅ Clear any previous graph data
tf.compat.v1.disable_eager_execution()
sess = tf.compat.v1.Session()

# ✅ Train Adversarial Debiasing model
debiased_model = AdversarialDebiasing(privileged_groups=[{'bias_type_encoded': 0}],
                                      unprivileged_groups=[{'bias_type_encoded': 1}],
                                      scope_name='debiased_model',
                                      debias=True,
                                      sess=sess)

# ✅ Initialize TensorFlow variables
sess.run(tf.compat.v1.global_variables_initializer())

# ✅ Train the model
debiased_model.fit(train_dataset)

# ✅ Evaluate fairness after debiasing
bias_metric_after = BinaryLabelDatasetMetric(train_dataset,
                                             privileged_groups=[{'bias_type_encoded': 0}],
                                             unprivileged_groups=[{'bias_type_encoded': 1}])

# ✅ Print fairness metric
print("✔ Disparate Impact After Debiasing:", bias_metric_after.disparate_impact())

import matplotlib.pyplot as plt
import seaborn as sns

# Function to plot bar and pie charts with gender labels
def plot_distribution(y, title_prefix):
    # Convert 0 → 'Female', 1 → 'Male'
    y_named = y.map({0: 'Female', 1: 'Male'})

    # Bar Chart
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.countplot(x=y_named, palette="pastel")
    plt.title(f'{title_prefix} - Gender Count')
    plt.xlabel('Gender')
    plt.ylabel('Number of Records')

    # Pie Chart
    plt.subplot(1, 2, 2)
    y_named.value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=sns.color_palette("pastel"))
    plt.title(f'{title_prefix} - Gender Percentage')
    plt.ylabel('')

    plt.tight_layout()
    plt.show()

# BEFORE BIAS REMOVAL
print("Before Bias Removal:")
print(y.value_counts())
plot_distribution(y, "Before Removing Bias")

# SMOTE: removing bias
smote = SMOTE()
X_smote, y_smote = smote.fit_resample(X, y)

# AFTER BIAS REMOVAL
print("After Bias Removal:")
print(pd.Series(y_smote).value_counts())
plot_distribution(pd.Series(y_smote), "After Removing Bias")

# Individual pie charts for each bias type's stereotype distribution
unique_biases = df_crows['bias_type'].unique()
n_cols = 3
n_rows = (len(unique_biases) + n_cols - 1) // n_cols

plt.figure(figsize=(15, 5*n_rows))
for i, bias in enumerate(unique_biases, 1):
    plt.subplot(n_rows, n_cols, i)
    subset = df_crows[df_crows['bias_type'] == bias]
    counts = subset['stereo_antistereo'].value_counts()
    plt.pie(counts, labels=counts.index, autopct='%1.1f%%',
            colors=['#ff9999', '#66b3ff'])
    plt.title(f'{bias} Bias', fontsize=12)
plt.suptitle('Stereotype Distribution by Bias Type', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()